{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Import Libraries\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "import os\n",
    "import gzip\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Load and Prepare Dataset (Optimized with Pandas)\n",
    "def load_and_prepare_data(filepath, sample_size=300):\n",
    "    # Load JSON data efficiently\n",
    "    print(\"Loading data...\")\n",
    "    start = time()\n",
    "    data = pd.read_json(filepath, lines=True)\n",
    "    print(f\"Loaded {len(data)} entries in {time()-start:.2f}s\")\n",
    "    \n",
    "    # Filter for required categories\n",
    "    categories = ['WELLNESS', 'ENTERTAINMENT', 'CRIME', 'POLITICS']\n",
    "    filtered_data = data[data['category'].isin(categories)].copy()\n",
    "    \n",
    "    # Sample evenly from each category\n",
    "    sampled_data = (filtered_data.groupby('category', group_keys=False, observed=True)\n",
    "                .apply(lambda x: x.sample(n=min(len(x), sample_size//len(categories)))))\n",
    "\n",
    "    # Create labels\n",
    "    sampled_data['label'] = sampled_data['category'].apply(\n",
    "        lambda x: 1 if x in ['WELLNESS', 'ENTERTAINMENT'] else 0\n",
    "    )\n",
    "    \n",
    "    # Test data (remaining entries)\n",
    "    test_data = filtered_data[~filtered_data.index.isin(sampled_data.index)]\n",
    "    \n",
    "    return sampled_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 3. Text Preprocessing (Vectorized with Pandas)\n",
    "basic_stopwords = {\n",
    "    # Articles/determiners\n",
    "    'the', 'a', 'an', 'some', 'any', 'all', 'both', 'each', 'every', 'either', 'neither',\n",
    "    \n",
    "    # Pronouns\n",
    "    'i', 'me', 'my', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "    'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself',\n",
    "    'they', 'them', 'their', 'theirs', 'themselves', 'this', 'that', 'these', 'those',\n",
    "    'who', 'whom', 'whose', 'which', 'what', 'whatever', 'whoever', 'whichever',\n",
    "    \n",
    "    # Common verbs (all forms)\n",
    "    'be', 'am', 'is', 'are', 'was', 'were', 'been', 'being',\n",
    "    'have', 'has', 'had', 'having',\n",
    "    'do', 'does', 'did', 'doing',\n",
    "    'can', 'could', 'shall', 'should', 'will', 'would', 'may', 'might', 'must',\n",
    "    \n",
    "    # Prepositions\n",
    "    'about', 'above', 'across', 'after', 'against', 'along', 'among', 'around', 'at',\n",
    "    'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'by',\n",
    "    'concerning', 'despite', 'down', 'during', 'except', 'for', 'from', 'in', 'inside',\n",
    "    'into', 'like', 'near', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over',\n",
    "    'past', 'regarding', 'since', 'through', 'throughout', 'to', 'toward', 'under',\n",
    "    'until', 'up', 'upon', 'with', 'within', 'without',\n",
    "    \n",
    "    # Conjunctions\n",
    "    'and', 'but', 'or', 'nor', 'for', 'yet', 'so', 'as', 'if', 'because', 'although',\n",
    "    'while', 'when', 'where', 'whether', 'since', 'unless', 'until', 'before', 'after',\n",
    "    \n",
    "    # Other common words\n",
    "    'there', 'here', 'very', 'just', 'not', 'no', 'yes', 'more', 'most', 'many', 'much',\n",
    "    'few', 'little', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very',\n",
    "    'well', 'also', 'even', 'just', 'now', 'then', 'again', 'already', 'still', 'always',\n",
    "    'never', 'often', 'usually', 'sometimes', 'rather', 'quite', 'perhaps', 'maybe',\n",
    "    'once', 'twice', 'new', 'old', 'good', 'bad', 'high', 'low', 'first', 'last',\n",
    "    'next', 'other', 'another', 'each', 'every', 'any', 'some', 'all', 'both', 'either',\n",
    "    'neither', 'few', 'many', 'several', 'such', 'what', 'which', 'whose', 'these',\n",
    "    'those', 'them', 'then', 'therefore', 'thus', 'hence', 'however', 'nevertheless',\n",
    "    'nonetheless', 'otherwise', 'accordingly', 'consequently', 'meanwhile','how', 'why','more', 'new', 'all', 'they',\n",
    "    \n",
    "    # Contractions\n",
    "    \"don't\", \"doesn't\", \"didn't\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"haven't\",\n",
    "    \"hasn't\", \"hadn't\", \"won't\", \"wouldn't\", \"can't\", \"couldn't\", \"shouldn't\",\n",
    "    \"mightn't\", \"mustn't\", \"i'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\",\n",
    "    \"i've\", \"you've\", \"we've\", \"they've\", \"i'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\",\n",
    "    \"i'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"let's\", \"that's\", \"who's\",\n",
    "    \"what's\", \"where's\", \"when's\", \"why's\", \"how's\"\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return [word for word in text.split() if word not in basic_stopwords]\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    print(\"Preprocessing text...\")\n",
    "    start = time()\n",
    "    df['processed'] = (df['headline'] + ' ' + df['short_description']).apply(preprocess_text)\n",
    "    print(f\"Preprocessing completed in {time()-start:.2f}s\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 4. Optimized NaÃ¯ve Bayes Implementation\n",
    "class OptimizedNaiveBayes:\n",
    "    def __init__(self, max_vocab_size=5000):\n",
    "        self.class_probs = None\n",
    "        self.word_probs = None\n",
    "        self.vocabulary = None\n",
    "        self.max_vocab_size = max_vocab_size  # Store the limit\n",
    "\n",
    "    \n",
    "    def train(self, data):\n",
    "        print(\"\\nTraining model...\")\n",
    "        start = time()\n",
    "        \n",
    "        # Calculate class probabilities\n",
    "        class_counts = data['label'].value_counts()\n",
    "        self.class_probs = (class_counts / class_counts.sum()).to_dict()\n",
    "        \n",
    "        # Keep only top 5000 most frequent words\n",
    "        word_counts = defaultdict(int)\n",
    "        for words in data['processed']:\n",
    "            for word in words:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "        self.vocabulary = [word for word, _ in \n",
    "                  sorted(word_counts.items(), key=lambda x: -x[1])[:5000]]\n",
    "\n",
    "        # Build vocabulary\n",
    "        all_words = [word for words in data['processed'] for word in words]\n",
    "        self.vocabulary = list(set(all_words))\n",
    "        \n",
    "        # Count words per class using vectorized operations\n",
    "        word_counts = {0: defaultdict(int), 1: defaultdict(int)}\n",
    "        class_totals = {0: 0, 1: 0}\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            cls = row['label']\n",
    "            for word in row['processed']:\n",
    "                word_counts[cls][word] += 1\n",
    "                class_totals[cls] += 1\n",
    "        \n",
    "        # Calculate probabilities with Laplace smoothing\n",
    "        self.word_probs = {0: {}, 1: {}}\n",
    "        v_size = len(self.vocabulary)\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            for word in self.vocabulary:\n",
    "                count = word_counts[cls].get(word, 0)\n",
    "                self.word_probs[cls][word] = (count + 1) / (class_totals[cls] + v_size)\n",
    "        \n",
    "        print(f\"Training completed in {time()-start:.2f}s\")\n",
    "    \n",
    "    def predict(self, text):\n",
    "        log_probs = {}\n",
    "        \n",
    "        for cls in [0, 1]:\n",
    "            # Start with log class probability\n",
    "            log_probs[cls] = np.log(self.class_probs[cls])\n",
    "            \n",
    "            # Add log word probabilities\n",
    "            for word in text:\n",
    "                if word in self.word_probs[cls]:\n",
    "                    log_probs[cls] += np.log(self.word_probs[cls][word])\n",
    "        \n",
    "        return max(log_probs.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    def save_model(self, filename='naive_bayes_model.pkl.gz', compressed=True):\n",
    "        model_data = {\n",
    "            'class_probs': self.class_probs,\n",
    "            'word_probs': dict(self.word_probs),\n",
    "            'vocabulary': self.vocabulary\n",
    "        }\n",
    "        \n",
    "        if compressed:\n",
    "            with gzip.open(filename, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "        else:\n",
    "            with open(filename, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "        \n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_model(filename='naive_bayes_model.pkl.gz'):\n",
    "    try:\n",
    "        # Detect compression from file extension\n",
    "        if filename.endswith('.gz'):\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        else:\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "        \n",
    "        model = OptimizedNaiveBayes()\n",
    "        model.class_probs = data['class_probs']\n",
    "        model.word_probs = defaultdict(dict, data['word_probs'])\n",
    "        model.vocabulary = data['vocabulary']\n",
    "        \n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file {filename} not found\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "def evaluate_model(model, test_data):\n",
    "    confusion_matrix = defaultdict(int)  # TP, FP, TN, FN\n",
    "    class_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        true_label = row['label']\n",
    "        pred = model.predict(row['processed'])\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        if true_label == 1 and pred == 1:\n",
    "            confusion_matrix['TP'] += 1\n",
    "        elif true_label == 1 and pred == 0:\n",
    "            confusion_matrix['FN'] += 1\n",
    "        elif true_label == 0 and pred == 1:\n",
    "            confusion_matrix['FP'] += 1\n",
    "        else:\n",
    "            confusion_matrix['TN'] += 1\n",
    "        \n",
    "        # Update class stats\n",
    "        class_stats[true_label]['total'] += 1\n",
    "        if pred == true_label:\n",
    "            class_stats[true_label]['correct'] += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (confusion_matrix['TP'] + confusion_matrix['TN']) / len(test_data)\n",
    "    precision = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FP'])\n",
    "    recall = confusion_matrix['TP'] / (confusion_matrix['TP'] + confusion_matrix['FN'])\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"               Predicted Like  Predicted Dislike\")\n",
    "    print(f\"Actual Like      {confusion_matrix['TP']:^12}      {confusion_matrix['FN']:^12}\")\n",
    "    print(f\"Actual Dislike   {confusion_matrix['FP']:^12}      {confusion_matrix['TN']:^12}\")\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Precision: {precision:.2%}\")\n",
    "    print(f\"Recall: {recall:.2%}\")\n",
    "    print(f\"F1 Score: {f1:.2%}\")\n",
    "    \n",
    "    for label, stats in class_stats.items():\n",
    "        print(f\"Class {'Like' if label else 'Dislike'} Accuracy: {stats['correct']/stats['total']:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 209527 entries in 0.83s\n",
      "Preprocessing text...\n",
      "Preprocessing completed in 0.00s\n",
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/hxvfktwx75s7mp8n_1frblw00000gn/T/ipykernel_5251/2634852532.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_data = (filtered_data.groupby('category', group_keys=False, observed=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 0.46s\n",
      "Model loaded from naive_bayes_model.pkl.gz\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Confusion Matrix:\n",
      "               Predicted Like  Predicted Dislike\n",
      "Actual Like         27017              8140    \n",
      "Actual Dislike       5875             33139    \n",
      "\n",
      "Overall Accuracy: 81.10%\n",
      "Precision: 82.14%\n",
      "Recall: 76.85%\n",
      "F1 Score: 79.40%\n",
      "Class Like Accuracy: 76.85%\n",
      "Class Dislike Accuracy: 84.94%\n",
      "Training labels: label\n",
      "0    150\n",
      "1    150\n",
      "Name: count, dtype: int64\n",
      "Test labels: label\n",
      "0    39014\n",
      "1    35157\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 'Like' words:\n",
      "[('one', 0.003609456776755098), ('help', 0.002165674066053059), ('life', 0.002165674066053059), ('want', 0.002165674066053059), ('make', 0.001985201227215304), ('day', 0.001804728388377549), ('people', 0.001804728388377549), ('health', 0.001804728388377549), ('weight', 0.0016242555495397943), ('healthy', 0.0016242555495397943)]\n",
      "\n",
      "Top 10 'Dislike' words:\n",
      "[('police', 0.0035889686437476386), ('trump', 0.0028333962976955043), ('man', 0.00207782395164337), ('trumps', 0.0018889308651303363), ('two', 0.0018889308651303363), ('arrested', 0.0018889308651303363), ('say', 0.0017000377786173025), ('time', 0.0017000377786173025), ('allegedly', 0.0017000377786173025), ('yearold', 0.0017000377786173025)]\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Train and Evaluate Model\n",
    "# Load and prepare data\n",
    "sampled_data, test_data = load_and_prepare_data('News_Category_Dataset_v3.json')\n",
    "\n",
    "# Preprocess text\n",
    "sampled_data = preprocess_dataframe(sampled_data)\n",
    "test_data = preprocess_dataframe(test_data)\n",
    "\n",
    "test_data['label'] = test_data['category'].apply(\n",
    "    lambda x: 1 if x in ['WELLNESS', 'ENTERTAINMENT'] else 0\n",
    ")\n",
    "\n",
    "# Try to load existing model, otherwise train new one\n",
    "nb = load_model()\n",
    "if nb is None:\n",
    "    nb = OptimizedNaiveBayes(max_vocab_size=5000)  # Initialize with vocab limit\n",
    "    nb.train(sampled_data)\n",
    "    nb.save_model()  # Save the newly trained model\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating model...\")\n",
    "evaluate_model(nb, test_data)\n",
    "\n",
    "print(\"Training labels:\", sampled_data['label'].value_counts())\n",
    "print(\"Test labels:\", test_data['label'].value_counts())\n",
    "\n",
    "# Top 10 \"Like\" words\n",
    "print(\"\\nTop 10 'Like' words:\")\n",
    "print(sorted(nb.word_probs[1].items(), key=lambda x: -x[1])[:10])\n",
    "\n",
    "# Top 10 \"Dislike\" words  \n",
    "print(\"\\nTop 10 'Dislike' words:\")\n",
    "print(sorted(nb.word_probs[0].items(), key=lambda x: -x[1])[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for: 'I love playing'\n",
      "Result: Like ðŸ‘\n",
      "\n",
      "Additional Predictions:\n",
      "New yoga techniques help reduce stress... -> Like ðŸ‘\n",
      "Political tensions rise in the Middle East... -> Dislike ðŸ‘Ž\n",
      "Celebrity wedding announcement... -> Like ðŸ‘\n",
      "Crime rate increases in urban areas... -> Dislike ðŸ‘Ž\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Make Predictions\n",
    "def predict_sentiment(text, model):\n",
    "    processed = preprocess_text(text)\n",
    "    return \"Like ðŸ‘\" if model.predict(processed) == 1 else \"Dislike ðŸ‘Ž\"\n",
    "\n",
    "# Example prediction\n",
    "test_article = \"I love playing\"\n",
    "print(f\"\\nPrediction for: '{test_article}'\")\n",
    "print(\"Result:\", predict_sentiment(test_article, nb))\n",
    "\n",
    "# Additional examples\n",
    "examples = [\n",
    "    \"New yoga techniques help reduce stress\",\n",
    "    \"Political tensions rise in the Middle East\",\n",
    "    \"Celebrity wedding announcement\",\n",
    "    \"Crime rate increases in urban areas\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional Predictions:\")\n",
    "for example in examples:\n",
    "    print(f\"{example[:50]}... -> {predict_sentiment(example, nb)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
